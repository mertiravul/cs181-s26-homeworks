\documentclass[submit]{../harvardml}
\usepackage{../common}

\course{CS1810-S26}
\assignment{Homework \#1}
\duedate{February 13, 2026 at 11:59 PM}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{graphicx}
\graphicspath{{./}{hw1/}}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{framed}
\usepackage{float}
\usepackage{ifthen}
\usepackage{bm}


\usepackage[mmddyyyy,hhmmss]{datetime}



\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

 \DeclareMathOperator*{\limover}{\overline{lim}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Solution environment
\usepackage{xcolor}
\newenvironment{solution}{
    \vspace{2mm}
    \color{blue}\noindent\textbf{Solution}:
}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{center}
  {\Large Regression}
\end{center}

\subsection*{Introduction}

This homework is on different three different forms of regression:
nearest neighbors regression, kernelized regression, and linear
regression.  We will discuss implementation and examine their
tradeoffs by implementing them on the same dataset, which consists of
temperature over the past 800,000 years taken from ice core samples.

The folder \verb|data| contains the data you will use for this
problem. There are two files:
\begin{itemize}
  \item \verb|earth_temperature_sampled_train.csv|
  \item \verb|earth_temperature_sampled_test.csv|
\end{itemize}

Each has two columns.  The first column is the age of the ice core
sample.  The second column is the approximate difference in a year's temperature (K)
from the average temperature of the 1,000 years preceding it. The temperatures were retrieved from ice cores in
Antarctica (Jouzel et al. 2007)\footnote{Retrieved from
  \url{https://www.ncei.noaa.gov/pub/data/paleo/icecore/antarctica/epica_domec/edc3deuttemp2007.txt}

  Jouzel, J., Masson-Delmotte, V., Cattani, O., Dreyfus, G., Falourd,
  S., Hoffmann, G., … Wolff, E. W. (2007). Orbital and Millennial
  Antarctic Climate Variability over the Past 800,000 Years.
  \emph{Science, 317}(5839), 793–796. doi:10.1126/science.1141038}.

The following is a snippet of the data file:

\begin{csv}
  # Age, Temperature
  399946,0.51
  409980,1.57
\end{csv}

\noindent And this is a visualization of the full dataset:
\begin{center}
  \includegraphics[width=.8\textwidth]{img_input/sample_graph}
\end{center}
\noindent


\textbf{Due to the large magnitude of the years, we will work in terms
  of thousands of years BCE in these problems.} This is taken care of
for you in the provided notebook.






\subsection*{Resources and Submission Instructions}

% The course textbook is not fully updated to the spring 2026 rendition of CS 1810, but if you want to take a look, it may still be helpful to see Sections 2.1-2.7 of the \href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{cs181-textbook notes}.

% We also encourage you to first read the
% \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{Bishop textbook}, particularly: Section 2.3 (Properties of Gaussian Distributions), Section 3.1 (Linear Basis Regression), and Section 3.3 (Bayesian Linear Regression). (Note that our notation is slightly different but the underlying mathematics remains the same!).

Please type your solutions after the corresponding problems using this \LaTeX\ template, and start each main problem on a new page.

Please submit the writeup PDF to the Gradescope assignment `HW1'. Remember to assign pages for each question.  \textbf{You must include any plots in your writeup PDF. }. Please submit your \LaTeX file and code files to the Gradescope assignment `HW1 - Supplemental.' The supplemental files will only be checked in special cases, e.g. honor code issues, etc. Your files should be named in the same way as we provide them in the repository, e.g. \texttt{hw1.pdf}, etc.

If you find that you are having trouble with the first couple problems, it may be helpful to refer to Section 0 notes and review some linear algebra and matrix calculus. 

\newpage 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[kNN and Kernels, 35pts]

You will now implement two non-parametric regressions to model temperatures over time.  
% For this problem, you will use the \textbf{same dataset as in Problem 1}.

\vspace{0.5cm}
\noindent\emph{Make sure to include all required plots in your PDF. Passing all test cases does not guarantee that your solution is correct, and we encourage you to write your own. }

\begin{enumerate}
\item 
 Recall that kNN uses a predictor of the form
\[
  f(x^*) = \frac{1}{k} \sum_n y_n \mathbb{I}(x_n \texttt{ is one of k-closest to } x^*),
\]
where $\mathbb{I}$ is an indicator variable. 
\begin{enumerate}

  \item The kNN implementation \textbf{has been provided for you} in the notebook. Run the cells to plot the results for $k=\{1, 3, N-1\}$, where $N$ is the size of the dataset. Describe how the fits change with $k$. Please include your plot in your solution PDF.

  \item Now, we will evaluate the quality of each model \emph{quantitatively} by computing the error on the provided test set. Write Python code to compute the test MSE for each value of $k$.  Report the values here. Which solution has the lowest MSE? 
  
\end{enumerate}

\item \textit{Kernel-based regression} techniques are another form of non-parametric regression. Consider a kernel-based
regressor of the form 
\begin{equation*}
  f_\tau(x^*) = \cfrac{\sum_{n} K_\tau(x_n,x^*) y_n}{\sum_n K_\tau(x_n, x^*)}
\end{equation*}
where $\mathcal{D}_\texttt{train} = \{(x_n,y_n)\}_{n = 1} ^N$ are the
training data points, and $x^*$ is the point for which you want to
make the prediction.  The kernel $K_\tau(x,x')$ is a function that
defines the similarity between two inputs $x$ and $x'$. A popular
choice of kernel is a function that decays as the distance between the
two points increases, such as
\begin{equation*}
  K_\tau(x,x') = \exp\left(-\frac{(x-x')^2}{\tau}\right)
\end{equation*}

where $\tau$ represents the square of the lengthscale (a scalar value that
dictates how quickly the kernel decays).  


\begin{enumerate}
    
  \item First, implement the \texttt{kernel\_regressor} function in the notebook, and plot your model for years in the range $800,000$ BC to $400,000$ BC at $1000$ year intervals for the following three values of $\tau$: $1, 50, 2500$. Since we're working in terms of thousands of years, this means you should plot $(x, f_\tau(x))$ for $x = 400, 401, \dots, 800$. \textbf{In no more than 10 lines}, describe how the fits change with $\tau$. Please include your plot in your solution PDF.

  \item Denote the test set as $\mathcal{D}_\texttt{test} = \{(x'_m, y'_m)\}_{m = 1} ^M$.  Write down the expression for the MSE of $f_\tau$ over the test set as a function of the training set and test set. Your answer may include $\{(x'_m, y'_m)\}_{m = 1} ^M$, $\{(x_n, y_n)\}_{n = 1} ^N$, and $K_\tau$, but not $f_\tau$.

    \item Compute the MSE on the provided test set for the three values of $\tau$.  Report the values here. Which model yields the lowest MSE? Conceptually, why is this the case? Why would choosing $\tau$ based on $\mathcal{D}_\texttt{train}$ rather than $\mathcal{D}_\texttt{test}$ be a bad idea? 

  \item Describe the time and space complexity of both kernelized regression and kNN with respect to the size of the training set $N$.  How, if at all, does the size of the model---everything that needs to be stored to make predictions---change with the size of the training set $N$?  How, if at all, do the number of computations required to make a prediction for some input $x^*$ change with the size of the training set $N$?.
  

  \item  What is the exact form of $\lim_{\tau \to 0 }f_\tau(x^*)$?
  \end{enumerate}
\end{enumerate}
\end{problem}

\newpage

\begin{solution}
\textbf{1(a)} The fits change as follows:
\begin{itemize}
    \item $k=1$: The predictor interpolates the training data exactly, passing through each point. The fit is very wiggly and sensitive to noise.
    \item $k=3$: The fit is smoother, averaging over 3 nearest neighbors. It captures local trends while reducing noise.
    \item $k=N-1$: The predictor averages almost all training points, yielding a nearly constant fit close to the mean of the training targets.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{img_output/p1.1a.png}
\end{center}

\textbf{1(b)} The test MSEs for each value of $k$ are:
\begin{itemize}
    \item $k=1$: 1.74
    \item $k=3$: 3.89
    \item $k=N-1$: 9.53
\end{itemize}
The solution with the lowest MSE is $k=1$.

\textbf{2(a)} As $\tau$ increases:
\begin{itemize}
    \item Small $\tau$ ($\tau=1$): The kernel decays quickly; predictions are highly local, tracking nearby points closely. The fit can be wiggly.
    \item Medium $\tau$ ($\tau=50$): A balance between local and global; smoother fit.
    \item Large $\tau$ ($\tau=2500$): The kernel is nearly flat; all training points contribute similarly. The fit approaches a constant (weighted average of targets).
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{img_output/p1.2a.png}
\end{center}

\textbf{2(b)} The MSE of $f_\tau$ over the test set is:
\[
\mathrm{MSE} = \frac{1}{M} \sum_{m=1}^{M} \left( y'_m - \frac{\sum_{n=1}^{N} K_\tau(x_n, x'_m) y_n}{\sum_{n=1}^{N} K_\tau(x_n, x'_m)} \right)^2
\]

\textbf{2(c)} The test MSEs for $\tau$ are: $\tau=1$: 1.95, $\tau=50$: 1.86, $\tau=2500$: 8.33. The model with $\tau=50$ yields the lowest MSE. Conceptually, $\tau=50$ balances bias and variance: too small $\tau$ overfits (high variance), too large underfits (high bias). Choosing $\tau$ based on $\mathcal{D}_\texttt{train}$ would be bad because it uses test information (data leakage) and leads to overly optimistic estimates.

\textbf{2(d)} \textbf{Time complexity}: Both require $O(N)$ operations per prediction (computing distances/kernels to all $N$ training points). \textbf{Space complexity}: Both store the full training set, so $O(N)$. \textbf{Model size}: Grows linearly with $N$ for both. \textbf{Prediction cost}: Grows linearly with $N$ for both.

\textbf{2(e)} As $\tau \to 0$, $K_\tau(x_n, x^*) \to 0$ for all $x_n \neq x^*$, and $K_\tau(x^*, x^*) = 1$ if $x^*$ equals some training point. In the limit, only the nearest training point(s) contribute. If $x^*$ has a unique nearest neighbor $x_n$, then $\lim_{\tau \to 0} f_\tau(x^*) = y_n$. If there are ties, the limit is the average of the $y$-values at the tied nearest points.
\end{solution}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Geometric Least Squares, 20pts]
    Linear regression can be understood using geometric intuition in $\mathbb{R}^n$. The design matrix $\mathbf X \in \mathbb{R}^{N\times D}$, with $N > D$, spans a subspace $C(\mathbf X)$, the column space of $\mathbf X$ (referred to in lecture as column span) which is a subspace of $\mathbb{R}^N$. If you wish to review the concept of a column space, consider visiting Section 0 notes. \\
    
    \noindent Fitting by linear regression, sometimes called \textit{ordinary least-squares} (OLS), is just projecting the observation vector $\mathbf y \in \mathbb{R}^N$ orthogonally onto that subspace. Lecture 2 slides provide a good graphic to visualize this, see the slide titled ``Geometric Interpretation.'' From lecture, we also learned that $\hat {\mathbf y}$ lives in $C(\mathbf X)$ and the residual $\mathbf r = {\mathbf y} - \hat {\mathbf y}$ lives in the orthogonal complement.

    \begin{enumerate}
    \item Let $\mathbf X \in \mathbb{R}^{N\times D}$ have full column rank $d$ and $\mathbf y \in \mathbb{R}^N$. Let the OLS estimator be $\mathbf w^*=(\mathbf X^\top \mathbf X)^{-1} X^\top \mathbf y$ and the fitted vector $\mathbf{\hat{y}} = \mathbf X \mathbf w^*$. Prove that $\hat {\mathbf y}$ is the \textit{orthogonal projection} of $y$ onto $C(\mathbf X)$. In other words, show that $\hat {\mathbf y} \in C(\mathbf X)$ and $\mathbf y - \hat {\mathbf y}$ is orthogonal to $C(\mathbf X)$. \textit{Hint: To show orthogonality, look at the gradient of $\mathcal L$, the loss, with respect to $\mathbf w$}.

    \item Prove that among all vectors in $C(\mathbf X)$, the fitted vector $\hat {\mathbf y}$ minimizes the Euclidean distance to $\mathbf y$. In other words, that for every vector $\mathbf v \in C(\mathbf X)$:
    \begin{equation*}
        \|\mathbf y - \hat {\mathbf y}\|_2^2 \leq \|\mathbf y - \mathbf v\|_2^2
    \end{equation*}
    Looking back at lecture, this is the formal proof of the phenomenon discussed in the image. \textit{Hint: For two vectors, $\mathbf v,\mathbf w$, if $\mathbf v$ is orthogonal to $\mathbf w$, denoted as $\mathbf v \perp \mathbf w$, then $\|\mathbf v-\mathbf w\|^2_2 = \|\mathbf v\|_2^2+\|\mathbf w\|^2_2$ (Pythagorean theorem).}

    \item In lecture, we defined the projection matrix, $\mathbf P = \mathbf X(\mathbf X^\top \mathbf X)^{-1} \mathbf X^\top$, which projects onto the subspace $C(\boldX)$. The matrix $\mathbf P$ is often called the \textit{hat matrix} because it maps $\mathbf y$ to its fitted values $\hat {\mathbf y} = \mathbf P \mathbf y$, i.e., it ``puts a hat" on $\mathbf y$. Prove the following properties of $\mathbf P$:
    \begin{itemize}
        \item Symmetry: $\mathbf P^\top = \mathbf P$
        \item Idempotence: $\mathbf P^2 = \mathbf P$
        \item Rank and Trace: $\mathrm{rank}(\mathbf P) = d$ and $\mathrm{trace}(\mathbf P) = d$.
    \end{itemize}
    % Also, provide geometric interpretation of the first two properties. 
    \textit{Hint: You may use the fact that any idempotent matrix has equal rank and trace. You do not need to prove this, but it may be helpful to think about why this is true.}
    
    \item Suppose you fit your model as in Problem 5.1. You observe that the \textbf{residual plot} exhibits a clear parabolic (U-shaped) pattern rather than random scatter around zero (as seen in lecture). Give a geometric interpretation of this phenomenon in terms of projection onto the column space of the design matrix. Also, explain how adding a quadratic basis function affects the geometry of the regression problem and the residuals.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.7]{img_input/residual_plot.png}
            \caption{An example residual plot with the input variable $x$ on the horizontal axis and residuals $y-\hat{y}$ on the vertical axis.}

          \end{figure}
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
\textbf{1} We show $\hat{y} \in C(X)$ and $y - \hat{y} \perp C(X)$.

First, $\hat{y} = X w^*$ is a linear combination of the columns of $X$, so $\hat{y} \in C(X)$.

For orthogonality: the OLS loss is $\mathcal{L}(w) = \frac{1}{2}\|y - Xw\|_2^2$. At the minimizer $w^*$, the gradient vanishes:
\[
\nabla_{w} \mathcal{L} = -X^\top(y - Xw^*) = -X^\top(y - \hat{y}) = 0.
\]
Thus $X^\top(y - \hat{y}) = 0$. For any $v \in C(X)$, we have $v = X a$ for some $a$. Hence
\[
(y - \hat{y})^\top v = (y - \hat{y})^\top X a = a^\top X^\top(y - \hat{y}) = 0.
\]
So $y - \hat{y}$ is orthogonal to every vector in $C(X)$.

\textbf{2} For any $v \in C(X)$, write $v = \hat{y} + u$ where $u \in C(X)$ (since $C(X)$ is a subspace). Then $y - v = (y - \hat{y}) - u$. We have $y - \hat{y} \perp u$ (both in appropriate spaces; $u \in C(X)$ and $y - \hat{y} \perp C(X)$). By the Pythagorean theorem:
\[
\|y - v\|_2^2 = \|y - \hat{y}\|_2^2 + \|u\|_2^2 \geq \|y - \hat{y}\|_2^2.
\]
Equality holds iff $u = 0$, i.e., $v = \hat{y}$. So $\hat{y}$ minimizes the distance.

\textbf{3} Let $P = X(X^\top X)^{-1}X^\top$.

\textit{Symmetry}: $P^\top = X(X^\top X)^{-1}X^\top = P$ (since $(X^\top X)^{-1}$ is symmetric).

\textit{Idempotence}: $P^2 = X(X^\top X)^{-1}X^\top X(X^\top X)^{-1}X^\top = X(X^\top X)^{-1}X^\top = P$.

\textit{Rank and trace}: $P$ projects onto $C(X)$, which has dimension $d$. So $\mathrm{rank}(P) = d$. For idempotent matrices, $\mathrm{rank}(P) = \mathrm{trace}(P)$, so $\mathrm{trace}(P) = d$.

\textbf{4} A parabolic (U-shaped) residual plot indicates that the true relationship has curvature (e.g., quadratic) that is not captured by the column space of $X$. The OLS fit projects $y$ onto $C(X)$; the residual $y - \hat{y}$ is the component of $y$ orthogonal to $C(X)$. When the true function is curved, this residual varies systematically with $x$ (e.g., positive for small and large $x$, negative in the middle), producing the U-shape. Adding a quadratic basis function expands $C(X)$ to include vectors that span quadratic behavior. The projection can then capture the curvature, and the residuals become more randomly scattered around zero.
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Basis Regression, 30pts]
    We now implement some linear regression models for the temperature. If we just directly use the data as given to us, we would only have a one dimensional input to our model, the year.  To create a more expressive linear model, we will introduce basis functions.
    
    \vspace{1em}
    
    \noindent\emph{Make sure to include all required plots in your PDF.}
    
    \begin{enumerate}
        \item We will first implement the four basis regressions below. Note that we introduce an addition transform $f$ (already into the provided notebook) to address concerns about numerical instabilities.
        
        \begin{enumerate}
            \item $\phi_j(x)= f(x)^j$ for $j=1,\ldots, 9$. $f(x) = \frac{x}{1.81 \cdot 10^{2}}.$
          
            \item $\phi_j(x) = \exp\left\{-\cfrac{(f(x)-\mu_j)^2}{5}\right\}$ for $\mu_j=\frac{j + 7}{8}$ with $j=1,\ldots, 9$. $f(x) = \frac{x}{4.00 \cdot 10^{2}}.$
          
            \item $\phi_j(x) =  \cos(f(x) / j)$ for $j=1, \ldots, 9$. $f(x) = \frac{x}{1.81}$.
          
            \item $\phi_j(x) = \cos(f(x) / j)$ for $j=1, \ldots, 49$. $f(x) = \frac{x}{1.81 \cdot 10^{-1}}$. \footnote{For the trigonometric bases (c) and (d), the periodic nature of cosine requires us to transform the data such that the lengthscale is within the periods of each element of our basis.}
        \end{enumerate}
    
        {\footnotesize *Note: Please make sure to add a bias term for all your basis functions above in your implementation of the \verb|make_basis|.}
    
        Let
        $$ \mathbf{\phi}(\mathbf{X}) = \begin{bmatrix}
            \mathbf{\phi}(x_1) \\
            \mathbf{\phi}(x_2) \\
            \vdots             \\
            \mathbf{\phi}(x_N) \\
        \end{bmatrix} \in \mathbb{R}^{N\times D}. $$
        You will complete the \verb|make_basis| function which must return $\phi(\mathbf{X})$ for each part (a) - (d). You do NOT need to submit this code in your \LaTeX writeup.
    
        Then, create a plot of the fitted regression line for each basis against a scatter plot of the training data. Boilerplate plotting code is provided in the notebook---you will only need to finish up a part of it. \textbf{All you need to include in your writeup for this part are these four plots.}
    
        \item Now we have trained each of our basis regressions. For each basis regression, compute the MSE on the test set. Discuss: do any of the bases seem to overfit? Underfit? Why?
    
        \item Briefly describe what purpose the transforms $\phi$ serve: why are they helpful?
    
        \item As in Problem 1, describe the space and time complexity of linear regression.  How does what is stored to compute predictions change with the size of the training set $N$ and the number of features $D$?  How does the computation needed to compute the prediction for a new input depend on the size of the training set $N$?  How do these complexities compare to those of the kNN and kernelized regressor?
    
        \item Briefly compare and constrast the different regressors: kNN, kernelized regression, and linear regression (with bases). Are some regressions clearly worse than others?  Is there one best regression?  How would you use the fact that you have these multiple regression functions?
    \end{enumerate}
      
    \noindent \textit{Note:} You may be concerned that we are using a different set of inputs $\mathbf{X}$ for each basis (a)-(d), since it could seem as though this prevents us from being able to directly compare the MSE of the models since we are using different data as input. But this is not an issue, since each transformation is considered as being a part of our model. This contrasts with transformations that cause the variance of the target $\mathbf{y}$ to be different  (such as standardization); in these cases the MSE can no longer be directly compared.
\end{problem}

\newpage

\begin{solution}
\textbf{1} The four plots are included below (from \texttt{p3.1.png}):

\begin{center}
\includegraphics[width=0.9\textwidth]{img_output/p3.1.png}
\end{center}

\textbf{2} The test MSEs for each basis are: (a) 7.96, (b) 8.71, (c) 5.97, (d) 58.92. Basis (c) has the lowest MSE. Basis (d) shows clear overfitting: it has 50 features and fits the training data very closely but generalizes poorly (MSE 58.92). Bases (a) and (b) may underfit: they have limited flexibility and higher MSE than (c). Basis (c) with 9 Fourier terms strikes a good balance.

\textbf{3} The basis transforms $\phi$ map the raw input $x$ into a higher-dimensional feature space. This allows the linear model $w^\top\phi(x)$ to capture nonlinear relationships while remaining linear in the weights, so we retain the analytic OLS solution. Different bases (polynomial, Gaussian, Fourier) provide different inductive biases for the underlying function.

\textbf{4} \textbf{Storage:} After training, we store only $w \in \mathbb{R}^D$, so $O(D)$---independent of $N$. \textbf{Prediction}: Computing $w^\top\phi(x^*)$ is $O(D)$, independent of $N$. In contrast, kNN and kernel regression store all $N$ training points ($O(N)$) and require $O(N)$ work per prediction. Linear regression is much more efficient at test time when $D \ll N$.

\textbf{5} No single regression is universally best. kNN and kernel regression are nonparametric and flexible but scale with $N$, linear regression with bases is parametric and efficient. On this dataset, kernel regression ($\tau=50$) and basis (c) perform well. In practice, one would use cross-validation to select among models and hyperparameters. Having multiple regressors allows ensemble methods or model selection based on validation performance.
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Probablistic View of Regression and Regularization, 30pts]
    Finally, we will explore an alternative view of linear regression to what was introduced in lecture. This view will be probabilistic. We will also introduce Bayesian regression under this probabilistic view. We will explore its connection to regularization for linear models, and then fit a regularized model to the temperature data. The probabilistic interpretation of linear regression is explored in more detail in the \href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{course notes} under section 2.6.2, but we have also tried to make this question self-contained with all necessary content.
    \\
    
    \noindent Recall that linear regression involves having $N$ labeled data points, say, $(\boldx_n,y_n)$ for $n\in\{1,\dots,N\}$. A probabilistic view of the linear regression problem supposes that the data actually came from a probabilistic model:
    \[y_n = \boldw^\top\boldx_n + \epsilon_n, \quad \epsilon_n \sim \mathcal{N}(0, \sigma^2).\]
    That is, we assume that there exists a set of coefficients $\boldw$ such that given data $\boldx_n$, the corresponding $y_n$ results from taking the $\boldw^\top\boldx_n$ and adding some random noise $\epsilon_n$. Here, we assume the noise is normally distributed with known mean and variance. The introduction of noise into the model accounts for the possibility of scatter, i.e., when the data does not literally follow a perfect line. It is shown in the aforementioned section of the course notes that under this probabilistic model, the data likelihood $p(\boldy|\boldw,\boldX)$ is maximized by $\boldw^* = (\boldX^\top\boldX)^{-1} \boldX^\top \boldy$, which, as we already saw in class, also minimizes the squared error. So, amazingly, the probabilistic view of regression leads to the view we saw in lecture, where we are trying to minimize a prediction error. \\
    
    \noindent Now, Bayesian regression takes this probablistic view a step further. You may recall that Bayesian statistics involves choosing a prior distribution for the parameters, here $\boldw$, based on our prior beliefs. So, in Bayesian regression, we additionally assume the weights are distributed $p(\boldw)$ and fit the weights $\boldw$ by maximizing the posterior likelihood
    \[ p(\boldw | \boldX, \boldy) = \frac{p(\bold y | \boldw, \boldX)p(\boldw)}{p(\boldy | \boldX)}. \]
    Note that since we maximize with respect to $\boldw$, it suffices to just maximize the numerator, while the denominator term does not need to be computed.
    
    \begin{enumerate}
        \item Suppose $\boldw \sim \mathcal{N}(\mathbf{0},\frac{\sigma^2}{\lambda}\boldI)$. Show that maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{ridge}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2 + \frac{\lambda}{2}||\boldw||_2^2.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{ridge}(\boldw)$ is exactly what regression with ridge regularization does.
        
        \textit{Hint:} You don't need to explicitly solve for the form of the maximizer/minimizer to show that the optimization problems are equivalent.
        
        \item Solve for the value of $\boldw$ that minimizes $\mathcal L_{ridge}(\boldw)$.
    
        \item The Laplace distribution has the PDF
       \[L(a,b) =\frac{1}{2b} \exp\left(-\frac{|x - a|}{b}\right)\]
        Show that if all $w_d \sim L\left(0,\frac{2\sigma^2}{\lambda}\right)$, maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{lasso}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2  + \frac{\lambda}{2}||\boldw||_1.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{lasso}(\boldw)$ is exactly what regression with LASSO regularization does.
    
        \item The LASSO estimator is the value of $\boldw$ that minimizes $\mathcal{L}_{lasso}(\boldw)$? It is very useful in certain real-world scenarios. Why is there no general closed form for the LASSO estimator?
    
        \item Since there is no general closed form for the LASSO estimator $\boldw$, we use numerical methods for estimating $\boldw$. One approach is to use \textit{coordinate descent}, which works as follows: 
        \begin{enumerate}
            \item Initialize $\boldw=\boldw_0$.
            \item For each $d=1, \ldots, D$ do the following 2 steps consecutively:
            \begin{enumerate}
                \item Compute $\rho_d = \tilde{\boldx}_d^\top(\boldy - (\boldX \boldw - w_d \tilde{\boldx}_d))$. We define $\tilde{\boldx}_d$ as the $d$-th column of $\boldX$.
    
                \item If $d=1$, set $w_1 = \frac{\rho_1}{||\tilde{\boldx}_1||^2_2}$. Otherwise if $d\ne 1$, compute $w_d = \frac{\text{sign}(\rho_d)\max\left\{|\rho_d|-\frac{\lambda}{2}, 0\right\}}{||\tilde{\boldx}_d||^2_2}$.
            \end{enumerate}
            \item Repeat step (b) until convergence or the maximum number of iterations is reached.
        \end{enumerate} 
    
        Implement the \texttt{find\_lasso\_weights} function according to the above algorithm, letting $\boldw_0$ be a vector of ones and the max number of iterations be 5000. Then, fit models with $\lambda=1, 10$ to basis (d) from Problem 3 and plot the predictions on the train set. Finally, compute the test MSE's. You will need to do some preprocessing, but a completed helper function for this is already provided. How do the graphs and errors compare to those for the unregularized (i.e., vanilla) basis (d) model? 
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
\textbf{1} With $w \sim \mathcal{N}(0, \frac{\sigma^2}{\lambda}I)$, the prior is $p(w) \propto \exp\left(-\frac{\lambda}{2\sigma^2}\|w\|_2^2\right)$. The likelihood is $p(y|w,X) \propto \exp\left(-\frac{1}{2\sigma^2}\|y - Xw\|_2^2\right)$. The posterior (up to constants) is:
\[
p(w|X,y) \propto p(y|w,X) p(w) \propto \exp\left(-\frac{1}{2\sigma^2}\|y - Xw\|_2^2 - \frac{\lambda}{2\sigma^2}\|w\|_2^2\right).
\]
Maximizing the posterior is equivalent to minimizing the negative log-posterior:
\[
\frac{1}{2\sigma^2}\|y - Xw\|_2^2 + \frac{\lambda}{2\sigma^2}\|w\|_2^2.
\]
Since $\sigma^2$ is constant, this is equivalent to minimizing $\mathcal{L}_{ridge}(w) = \frac{1}{2}\|y - Xw\|_2^2 + \frac{\lambda}{2}\|w\|_2^2$.

\textbf{2} Setting $\nabla_{w} \mathcal{L}_{ridge} = -X^\top(y - Xw) + \lambda w = 0$:
\[
X^\top Xw + \lambda w = X^\top y \quad \Rightarrow \quad (X^\top X + \lambda I)w = X^\top y.
\]
Thus $w^* = (X^\top X + \lambda I)^{-1}X^\top y$.

\textbf{3} For $w_d \sim L(0, \frac{2\sigma^2}{\lambda})$, the prior PDF is $p(w_d) = \frac{\lambda}{4\sigma^2} \exp\left(-\frac{\lambda|w_d|}{2\sigma^2}\right)$. So $p(w) \propto \exp\left(-\frac{\lambda}{2\sigma^2}\|w\|_1\right)$. The posterior is:
\[
p(w|X,y) \propto \exp\left(-\frac{1}{2\sigma^2}\|y - Xw\|_2^2 - \frac{\lambda}{2\sigma^2}\|w\|_1\right).
\]
Maximizing is equivalent to minimizing $\mathcal{L}_{lasso}(w) = \frac{1}{2}\|y - Xw\|_2^2 + \frac{\lambda}{2}\|w\|_1$.

\textbf{4} The LASSO objective includes the $\ell_1$ norm $\|w\|_1$, which is non-differentiable at $w_d = 0$ for any coordinate $d$. There is no closed-form solution like ridge regression because setting the gradient to zero does not yield a well-defined system; the subgradient conditions lead to piecewise solutions (soft-thresholding) that depend on the data in a non-linear way.

\textbf{5} The test MSEs are: $\lambda=1$: 30.06, $\lambda=10$: 15.62. The unregularized basis (d) model had test MSE 58.92. Both LASSO models improve over the unregularized (d): stronger regularization ($\lambda=10$) yields a smoother fit and lower test MSE than $\lambda=1$. The graphs show that LASSO produces smoother curves than the unregularized (d) model, which was severely overfitting. Regularization reduces overfitting by shrinking weights toward zero.

\begin{center}
  \includegraphics[width=0.7\textwidth]{img_output/p4.5.png}
\end{center}

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Name and Calibration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{Name}: Mert Iravul

\textbf{Collaborators and Resources}: 

\end{document}
